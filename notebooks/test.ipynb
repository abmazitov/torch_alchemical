{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "from ase.io import read\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from torch_alchemical.data import AtomisticDataset\n",
    "from torch_alchemical.models import AlchemicalModel\n",
    "from torch_alchemical.utils import (\n",
    "    get_list_of_unique_atomic_numbers,\n",
    "    get_species_coupling_matrix,\n",
    ")\n",
    "from torch_alchemical.transforms import CompositionFeatures, NeighborList\n",
    "from torch.nn.functional import mse_loss, l1_loss\n",
    "\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "N_PSEUDO = 4\n",
    "CUTOFF = 5.0\n",
    "HIDDEN_SIZE = 80\n",
    "RADIAL_BASIS_CUTOFF = 250.0\n",
    "PS_BASIS_CUTOFF = 180.0\n",
    "ENERGIES_WEIGHT = 1.0\n",
    "FORCES_WEIGHT = 1.0\n",
    "TRAIN_VAL_TEST = [0.8, 0.1, 0.1]\n",
    "\n",
    "\n",
    "def train_test_split(dataset, lengths, shuffle=True):\n",
    "    train_val_test = [length / sum(lengths) for length in lengths]\n",
    "    if shuffle:\n",
    "        return torch.utils.data.random_split(dataset, lengths)\n",
    "    else:\n",
    "        train_set_indices = range(0, int(train_val_test[0] * len(dataset)))\n",
    "        train_set = torch.utils.data.Subset(\n",
    "            dataset, train_set_indices\n",
    "        )\n",
    "        val_set_indices = range(\n",
    "            int(train_val_test[0] * len(dataset)), int(sum(lengths[:2])*len(dataset))\n",
    "        )\n",
    "        val_set = torch.utils.data.Subset(\n",
    "            dataset, val_set_indices\n",
    "        )\n",
    "        test_set_indices = range(\n",
    "            int(sum(lengths[:2])*len(dataset)), int(sum(lengths)*len(dataset))\n",
    "        )\n",
    "\n",
    "        test_set = torch.utils.data.Subset(\n",
    "            dataset, test_set_indices\n",
    "        )\n",
    "        return train_set, val_set, test_set\n",
    "\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, model, energies_weight: float, forces_weight: float):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.energies_weight = energies_weight\n",
    "        self.forces_weight = forces_weight\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    def initialize_composition_layer_weights(self, model, datamodule):\n",
    "        assert hasattr(model, \"composition_layer\")\n",
    "        dataset = datamodule.train_dataset\n",
    "        composition_layer = model.composition_layer\n",
    "        compositions = torch.cat([data.composition for data in dataset], dim=0)\n",
    "        compositions = torch.cat(\n",
    "            (torch.ones(len(dataset)).view(-1, 1), compositions), dim=1\n",
    "        )  # bias\n",
    "        energies = torch.cat([data.energies.view(1, -1) for data in dataset], dim=0)\n",
    "        weights = torch.linalg.lstsq(compositions, energies).solution\n",
    "        composition_layer.weight = torch.nn.Parameter(weights[1:].T.contiguous(), requires_grad=True)\n",
    "        composition_layer.bias = torch.nn.Parameter(weights[0].contiguous(), requires_grad=True)\n",
    "        print(\"Composition layer weights are initialized with least squares solution\")\n",
    "\n",
    "    def initialize_combining_matrix(self, model, datamodule):\n",
    "        assert hasattr(model, \"ps_features_layer\")\n",
    "        model.ps_features_layer.spex_calculator.combination_matrix.weight = torch.nn.Parameter(\n",
    "            get_species_coupling_matrix(datamodule.unique_numbers, N_PSEUDO).contiguous(), requires_grad=True\n",
    "        )\n",
    "        print(\"Combinining matrix is initialized manually\")\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.epoch_loss = 0.0\n",
    "        self.train_energies_mae = 0.0\n",
    "        self.train_forces_mae = 0.0\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        optimizer = self.optimizers()\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            predicted_energies, predicted_forces = self.model(batch)\n",
    "            target_energies = batch.energies\n",
    "            target_forces = batch.forces\n",
    "            energies_loss = self.energies_weight * mse_loss(\n",
    "                predicted_energies.flatten(), target_energies.flatten(), reduction=\"sum\"\n",
    "            )\n",
    "            forces_loss = self.forces_weight * mse_loss(\n",
    "                predicted_forces.flatten(), target_forces.flatten(), reduction=\"sum\"\n",
    "            )\n",
    "            loss = energies_loss + forces_loss\n",
    "            self.manual_backward(loss)\n",
    "            self.epoch_loss = loss.detach().item()\n",
    "            self.train_energies_mae = l1_loss(\n",
    "                predicted_energies.detach().flatten(),\n",
    "                target_energies.detach().flatten(),\n",
    "            )\n",
    "            self.train_forces_mae = l1_loss(\n",
    "                predicted_forces.flatten(), target_forces.flatten()\n",
    "            )\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    # def training_step(self, batch, batch_idx):\n",
    "    #     predicted_energies, predicted_forces = self.model(batch)\n",
    "    #     target_energies = batch.energies\n",
    "    #     target_forces = batch.forces\n",
    "    #     energies_loss = self.energies_weight * mse_loss(\n",
    "    #         predicted_energies.flatten(), target_energies, reduction=\"sum\"\n",
    "    #     )\n",
    "    #     forces_loss = self.forces_weight * mse_loss(\n",
    "    #         predicted_forces, target_forces, reduction=\"sum\"\n",
    "    #     )\n",
    "    #     loss = energies_loss + forces_loss\n",
    "    #     self.epoch_loss += loss.detach().item()\n",
    "    #     predicted_energies = predicted_energies.detach()\n",
    "    #     predicted_forces = predicted_forces.detach()\n",
    "    #     self.train_energies_mae += l1_loss(\n",
    "    #         predicted_energies.flatten(), target_energies\n",
    "    #     )\n",
    "    #     self.train_forces_mae += l1_loss(predicted_forces, target_forces)\n",
    "\n",
    "    #     return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        num_batches = len(self.trainer.datamodule.train_dataloader())\n",
    "        epoch_loss = self.epoch_loss\n",
    "        train_energies_mae = self.train_energies_mae / num_batches\n",
    "        train_forces_mae = self.train_forces_mae / num_batches\n",
    "        print(\n",
    "            f\"Loss: {epoch_loss:.4f}, Train Energies MAE: {train_energies_mae:.4f}, Train Forces MAE: {train_forces_mae:.4f}\"\n",
    "        )\n",
    "        self.log(\"loss\", epoch_loss)\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        torch.set_grad_enabled(True)\n",
    "        self.val_energies_mae = 0.0\n",
    "        self.val_forces_mae = 0.0\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        predicted_energies, predicted_forces = self.model(batch, training=False)\n",
    "        predicted_energies = predicted_energies.flatten().detach()\n",
    "        predicted_forces = predicted_forces.detach()\n",
    "        target_energies = batch.energies\n",
    "        target_forces = batch.forces\n",
    "        energies_mae = l1_loss(predicted_energies, target_energies)\n",
    "        forces_mae = l1_loss(predicted_forces, target_forces)\n",
    "        self.val_energies_mae += energies_mae.item()\n",
    "        self.val_forces_mae += forces_mae.item()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        num_batches = len(self.trainer.datamodule.val_dataloader())\n",
    "        val_energies_mae = self.val_energies_mae / num_batches\n",
    "        val_forces_mae = self.val_forces_mae / num_batches\n",
    "        self.log(\"val_energies_mae\", val_energies_mae)\n",
    "        self.log(\"val_forces_mae\", val_forces_mae)\n",
    "        print(\n",
    "            f\"Val Energies MAE: {val_energies_mae:.4f}, Val Forces MAE: {val_forces_mae:.4f}\"\n",
    "        )\n",
    "\n",
    "    # def configure_optimizers(self):\n",
    "    #     optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    #     scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    #         optimizer, gamma=0.99, verbose=True\n",
    "    #     )\n",
    "    #     return [optimizer], [scheduler]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.LBFGS(\n",
    "            self.parameters(), lr=0.05, history_size=128, line_search_fn=\"strong_wolfe\"\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "class LitDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        frames_path: str,\n",
    "        batch_size: int,\n",
    "        shuffle: bool = True,\n",
    "        verbose: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.frames_path = frames_path\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.frames = read(self.frames_path, \":\")[:1250]\n",
    "        self.unique_numbers = get_list_of_unique_atomic_numbers(self.frames)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if stage in (None, \"prepare\"):\n",
    "            transforms = [\n",
    "                CompositionFeatures(self.unique_numbers),\n",
    "                NeighborList(cutoff_radius=CUTOFF),\n",
    "            ]\n",
    "            dataset = AtomisticDataset(\n",
    "                self.frames,\n",
    "                target_properties=[\"energies\", \"forces\"],\n",
    "                transforms=transforms,\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "            (\n",
    "                self.train_dataset,\n",
    "                self.val_dataset,\n",
    "                self.test_dataset,\n",
    "            ) = train_test_split(dataset, TRAIN_VAL_TEST, shuffle=self.shuffle)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        batch_size = self.batch_size\n",
    "        if self.batch_size == \"len\":\n",
    "            batch_size = len(self.train_dataset)\n",
    "        dataloader = DataLoader(\n",
    "            self.train_dataset, batch_size=batch_size, shuffle=self.shuffle\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        batch_size = self.batch_size\n",
    "        if self.batch_size == \"len\":\n",
    "            batch_size = len(self.val_dataset)\n",
    "        dataloader = DataLoader(\n",
    "            self.val_dataset, batch_size=batch_size, shuffle=self.shuffle\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        batch_size = self.batch_size\n",
    "        if self.batch_size == \"len\":\n",
    "            batch_size = len(self.test_dataset)\n",
    "        dataloader = DataLoader(\n",
    "            self.test_dataset, batch_size=batch_size, shuffle=self.shuffle\n",
    "        )\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equistore\n",
    "import torch\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "\n",
    "from torch_alchemical.nn import (\n",
    "    Linear,\n",
    "    PowerSpectrumFeatures,\n",
    "    RadialSpectrumFeatures,\n",
    "    SiLU,\n",
    ")\n",
    "\n",
    "\n",
    "class AlchemicalModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_sizes: int,\n",
    "        output_size: int,\n",
    "        unique_numbers: Union[list, np.ndarray],\n",
    "        cutoff: float,\n",
    "        basis_cutoff_radial_spectrum: float,\n",
    "        basis_cutoff_power_spectrum: float,\n",
    "        num_pseudo_species: int = None,\n",
    "        device: torch.device = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.composition_layer = torch.nn.Linear(len(unique_numbers), output_size)\n",
    "        self.rs_features_layer = RadialSpectrumFeatures(\n",
    "            unique_numbers, cutoff, basis_cutoff_radial_spectrum, device\n",
    "        )\n",
    "        self.ps_features_layer = PowerSpectrumFeatures(\n",
    "            unique_numbers,\n",
    "            cutoff,\n",
    "            basis_cutoff_power_spectrum,\n",
    "            num_pseudo_species,\n",
    "            device,\n",
    "        )\n",
    "        rs_input_size = self.rs_features_layer.num_features\n",
    "        ps_input_size = self.ps_features_layer.num_features\n",
    "        self.rs_linear = Linear(rs_input_size, output_size)\n",
    "        self.ps_linear = Linear(ps_input_size, output_size)\n",
    "        layer_size = [ps_input_size] + hidden_sizes\n",
    "        layers = []\n",
    "        for layer_index in range(1, len(layer_size)):\n",
    "            layers.append(Linear(layer_size[layer_index - 1], layer_size[layer_index]))\n",
    "            layers.append(SiLU())\n",
    "        layers.append(Linear(layer_size[-1], output_size))\n",
    "        self.nn = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, batch, training=True):\n",
    "        energies = self.composition_layer(batch.composition)\n",
    "        rs = self.rs_features_layer(batch)\n",
    "        ps = self.ps_features_layer(batch)\n",
    "        rsl = self.rs_linear(rs)\n",
    "        psl = self.ps_linear(ps)\n",
    "        energies += (\n",
    "            equistore.sum_over_samples(rsl.keys_to_samples(\"a_i\"), [\"center\", \"a_i\"])\n",
    "            .block()\n",
    "            .values\n",
    "        )\n",
    "        energies += (\n",
    "            equistore.sum_over_samples(psl.keys_to_samples(\"a_i\"), [\"center\", \"a_i\"])\n",
    "            .block()\n",
    "            .values\n",
    "        )\n",
    "        psnn = self.nn(ps)\n",
    "        energies += (\n",
    "            equistore.sum_over_samples(psnn.keys_to_samples(\"a_i\"), [\"center\", \"a_i\"])\n",
    "            .block()\n",
    "            .values\n",
    "        )\n",
    "        forces = -torch.autograd.grad(\n",
    "            energies,\n",
    "            batch.pos,\n",
    "            grad_outputs=torch.ones_like(energies),\n",
    "            create_graph=training,\n",
    "            retain_graph=training,\n",
    "        )[0]\n",
    "        return energies, forces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 100/100 [00:05<00:00, 18.04it/s]\n"
     ]
    }
   ],
   "source": [
    "datamodule = LitDataModule(\"../data/hea_samples_bulk.xyz\", batch_size=\"len\", shuffle=False, verbose=True)\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup(stage=\"prepare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(datamodule.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_max = 4\n",
      "[2 2 1 1 1]\n",
      "Normalization check (needs to be close to 1): 0.9999999999999999\n",
      "l_max = 4\n",
      "[2 2 1 1 1]\n",
      "Normalization check (needs to be close to 1): 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "model = AlchemicalModel(\n",
    "    hidden_sizes = [80, 80],\n",
    "    output_size = 1,\n",
    "    unique_numbers = datamodule.unique_numbers,\n",
    "    cutoff = CUTOFF,\n",
    "    basis_cutoff_radial_spectrum = 80,\n",
    "    basis_cutoff_power_spectrum = 80,\n",
    "    num_pseudo_species = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mazitov/apps/anaconda3/envs/alch-learning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/mazitov/apps/anaconda3/envs/alch-learning/lib/ ...\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "litmodel = LitModel(\n",
    "    model, energies_weight=ENERGIES_WEIGHT, forces_weight=FORCES_WEIGHT\n",
    ")\n",
    "trainer = pl.Trainer(max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composition layer weights are initialized with least squares solution\n",
      "Combinining matrix is initialized manually\n"
     ]
    }
   ],
   "source": [
    "litmodel.initialize_composition_layer_weights(litmodel.model, datamodule)\n",
    "litmodel.initialize_combining_matrix(litmodel.model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = trainer.optimizers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.defaults['max_iter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mazitov/apps/anaconda3/envs/alch-learning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/mazitov/apps/anaconda3/envs/alch-learning/lib/ ...\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | AlchemicalModel | 21.2 K\n",
      "------------------------------------------\n",
      "21.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "21.2 K    Total params\n",
      "0.085     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef965d96b74f4ce788cc09aeaf618c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mazitov/apps/anaconda3/envs/alch-learning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Energies MAE: 18.9686, Val Forces MAE: 1.7095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mazitov/apps/anaconda3/envs/alch-learning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/mazitov/apps/anaconda3/envs/alch-learning/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594fa78a73734e20bcbe46e703458496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mazitov/apps/anaconda3/envs/alch-learning/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(litmodel, datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alch-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "348f37184c0ca061c4e7594a7f9223978a0641079a36c90deaeb02f74b52f9f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
